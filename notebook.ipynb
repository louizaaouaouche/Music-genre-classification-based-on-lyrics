{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD201 : MINING OF LARGE DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUSIC GENRE CLASSIFICATION USING SONG LYRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CONSTRUCTION OF THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source of the dataset is a csv file : a result of a scrapping work through [GENIUS](https://genius.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 RAW DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset from csv file\n",
    "data = pd.read_csv(\"lyrics.csv\", sep='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['artist', 'title', 'lyrics', 'genre', 'url'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns of dataset\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>genre</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eminem</td>\n",
       "      <td>Rap God</td>\n",
       "      <td>Rap God Lyrics\\r\\n\"Look, I was gonna go easy o...</td>\n",
       "      <td>rap</td>\n",
       "      <td>https://genius.com/Eminem-rap-god-lyrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardi B</td>\n",
       "      <td>WAP</td>\n",
       "      <td>WAP Lyrics\\r\\nWhores in this house\\r\\nThere's ...</td>\n",
       "      <td>rap</td>\n",
       "      <td>https://genius.com/Cardi-b-wap-lyrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kendrick Lamar</td>\n",
       "      <td>HUMBLE.</td>\n",
       "      <td>HUMBLE. Lyrics\\r\\nNobody pray for me\\r\\nIt bee...</td>\n",
       "      <td>rap</td>\n",
       "      <td>https://genius.com/Kendrick-lamar-humble-lyrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Migos</td>\n",
       "      <td>Bad and Boujee</td>\n",
       "      <td>Bad and Boujee Lyrics\\r\\nYou know, young rich ...</td>\n",
       "      <td>rap</td>\n",
       "      <td>https://genius.com/Migos-bad-and-boujee-lyrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drake</td>\n",
       "      <td>God's Plan</td>\n",
       "      <td>God’s Plan Lyrics\\r\\nAnd they wishin' and wish...</td>\n",
       "      <td>rap</td>\n",
       "      <td>https://genius.com/Drake-gods-plan-lyrics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           artist           title  \\\n",
       "0          Eminem         Rap God   \n",
       "1         Cardi B             WAP   \n",
       "2  Kendrick Lamar         HUMBLE.   \n",
       "3           Migos  Bad and Boujee   \n",
       "4           Drake      God's Plan   \n",
       "\n",
       "                                              lyrics genre  \\\n",
       "0  Rap God Lyrics\\r\\n\"Look, I was gonna go easy o...   rap   \n",
       "1  WAP Lyrics\\r\\nWhores in this house\\r\\nThere's ...   rap   \n",
       "2  HUMBLE. Lyrics\\r\\nNobody pray for me\\r\\nIt bee...   rap   \n",
       "3  Bad and Boujee Lyrics\\r\\nYou know, young rich ...   rap   \n",
       "4  God’s Plan Lyrics\\r\\nAnd they wishin' and wish...   rap   \n",
       "\n",
       "                                               url  \n",
       "0         https://genius.com/Eminem-rap-god-lyrics  \n",
       "1            https://genius.com/Cardi-b-wap-lyrics  \n",
       "2  https://genius.com/Kendrick-lamar-humble-lyrics  \n",
       "3   https://genius.com/Migos-bad-and-boujee-lyrics  \n",
       "4        https://genius.com/Drake-gods-plan-lyrics  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting the content of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6858, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist    object\n",
       "title     object\n",
       "lyrics    object\n",
       "genre     object\n",
       "url       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# types of data \n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 RAW DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unwanted columns\n",
    "data = data.drop(columns = ['artist','title','url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lyrics    25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Missing values\n",
    "data.isna().sum()[data.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lyrics    0\n",
       "genre     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check missing values (check is dropping went right)\n",
    "data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the \"[title]-Lyrics\" from the beginning of each lyrics and \"[number xxx]Embed\" from the end and the non-english lyrics\n",
    "\n",
    "for i in range(len(data['lyrics'])):\n",
    "    if(detect(data['lyrics'][i]) != 'en'):\n",
    "        data.drop([i])\n",
    "\n",
    "    if(re.findall(r'\\d+.*Embed.*', data['lyrics'][i])!=[]):\n",
    "        data['lyrics'][i] = data['lyrics'][i].split(re.findall(r'\\d+.*Embed.*', data['lyrics'][i])[0])[0]\n",
    "\n",
    "    if (re.findall(r'\\bLyrics\\b', data['lyrics'][i])!=[]):\n",
    "        data['lyrics'][i] = data['lyrics'][i].split('Lyrics')[1]\n",
    "    data['lyrics'][i] = data['lyrics'][i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house (hol' up)\r\n",
      "i said certified freak, seven days a week\r\n",
      "wet-ass pussy, make that pullout game weak, woo (ah)\r\n",
      "\r\n",
      "yeah, yeah, yeah, yeah\r\n",
      "yeah, you fuckin' with some wet-ass pussy\r\n",
      "bring a bucket and a mop for this wet-ass pussy\r\n",
      "give me everything you got for this wet-ass pussy\r\n",
      "\r\n",
      "beat it up, nigga, catch a charge\r\n",
      "extra large and extra hard\r\n",
      "put this pussy right in your face\r\n",
      "swipe your nose like a credit card\r\n",
      "hop on top, i wanna ride\r\n",
      "i do a kegel while it's inside\r\n",
      "spit in my mouth, look in my eyes\r\n",
      "this pussy is wet, come take a dive\r\n",
      "tie me up like i'm surprised\r\n",
      "let's roleplay, i'll wear a disguise\r\n",
      "i want you to park that big mack truck right in this little garage\r\n",
      "make it cream, make me scream\r\n",
      "out in public, make a scene\r\n",
      "i don't cook, i don't clean\r\n",
      "but let me tell you how i got this ring (ayy, ayy)\r\n",
      "you might also like\r\n",
      "gobble me, swallow me, drip down the side of me (yeah)\r\n",
      "quick, jump out 'fore you let it get inside of me (yeah)\r\n",
      "i tell him where to put it, never tell him where i'm 'bout to be (huh)\r\n",
      "i'll run down on him 'fore i have a nigga runnin' me (pow, pow, pow)\r\n",
      "talk your shit, bite your lip (yeah)\r\n",
      "ask for a car while you ride that dick (while you ride that dick)\r\n",
      "you really ain't never gotta fuck him for a thang (yeah)\r\n",
      "he already made his mind up 'fore he came (ayy, ah)\r\n",
      "now get your boots and your coat for this wet-ass pussy (ah, ah, ah)\r\n",
      "he bought a phone just for pictures of this wet-ass pussy (click, click, click)\r\n",
      "paid my tuition just to kiss me on this wet-ass pussy (mwah, mwah, mwah)\r\n",
      "now make it rain if you wanna see some wet-ass pussy (yeah, yeah)\r\n",
      "\r\n",
      "look, i need a hard hitter, need a deep stroker\r\n",
      "need a henny drinker, need a weed smoker\r\n",
      "not a garter snake, i need a king cobra\r\n",
      "with a hook in it, hope it lean over\r\n",
      "he got some money, then that's where i'm headed\r\n",
      "pussy a1 just like his credit\r\n",
      "he got a beard, well, i'm tryna wet it\r\n",
      "i let him taste it, now he diabetic\r\n",
      "i don't wanna spit, i wanna gulp\r\n",
      "i wanna gag, i wanna choke\r\n",
      "i want you to touch that lil' dangly thing that swing in the back of my throat\r\n",
      "my head game is fire, punani dasani\r\n",
      "it's goin' in dry and it's comin' out soggy\r\n",
      "i ride on that thing like the cops is behind me (yeah, ah)\r\n",
      "i spit on his mic and now he tryna sign me, woo\r\n",
      "your honor, i'm a freak bitch, handcuffs, leashes\r\n",
      "switch my wig, make him feel like he cheatin'\r\n",
      "put him on his knees, give him somethin' to believe in\r\n",
      "never lost a fight, but i'm lookin' for a beatin' (ah)\r\n",
      "in the food chain, i'm the one that eat ya\r\n",
      "if he ate my ass, he's a bottom-feeder\r\n",
      "big d stand for big demeanor\r\n",
      "i could make ya bust before i ever meet ya\r\n",
      "if it don't hang, then he can't bang\r\n",
      "you can't hurt my feelings, but i like pain\r\n",
      "if he fuck me and ask \"whose is it?\"\r\n",
      "when i ride the dick, i'ma spell my name, ah\r\n",
      "\r\n",
      "yeah, yeah, yeah\r\n",
      "yeah, you fuckin' with some wet-ass pussy\r\n",
      "bring a bucket and a mop for this wet-ass pussy\r\n",
      "give me everything you got for this wet-ass pussy\r\n",
      "now from the top, make it drop, that's some wet-ass pussy\r\n",
      "now get a bucket and a mop, that's some wet-ass pussy\r\n",
      "i'm talkin' wap, wap, wap, that's some wet-ass pussy\r\n",
      "macaroni in a pot, that's some wet-ass pussy, huh\r\n",
      "\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\r\n",
      "there's some whores in this house\n"
     ]
    }
   ],
   "source": [
    "#Checking the data cleaning on a random song lyrics\n",
    "print(data['lyrics'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rap\n"
     ]
    }
   ],
   "source": [
    "# Common repeated words by genre\n",
    "for the_genre in data.genre.unique():\n",
    "    print(the_genre)\n",
    "    print(data.loc[data['genre'] == the_genre].lyrics.str.split(expand=True).stack().value_counts()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation and stopwords from lyrics\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#stopwords = stopwords.words('english')\n",
    "stopwords = [\"'d\",\"'m\",\"'s\",\"'ve\",\"'re\",\"'ll\",\"'cause\",\"'bout\", \n",
    "             \"a\", \"able\", \"about\", \"above\", \"across\", \"actually\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"all\", \"almost\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"an\", \"and\", \"any\", \"anybody\", \"anyhow\",  \"apart\", \"are\", \"aren\", \"arent\", \"aren't\", \"around\", \"as\", \"at\",  \"aw\", \"away\",\n",
    "             \"b\", \"back\", \"be\" , \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\",\"both\", \"bottom\", \"but\", \"by\", \n",
    "             \"c\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\",  \"cit\", \"clearly\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\",\"could\", \"couldn\", \"couldnt\", \"couldn't\", \"currently\", \n",
    "             \"d\", \"date\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\",\"doin'\", \"don\", \"done\", \"don't\", \"down\", \"due\", \"during\", \n",
    "             \"e\", \"each\", \"ei\", \"eight\", \"eighty\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"especially\",  \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\",  \"except\", \"ey\", \n",
    "             \"f\", \"far\", \"fc\", \"few\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"first\", \"five\", \"fix\", \"fo\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\",  \"from\", \"front\", \"fu\", \"full\", \"further\", \"furthermore\",\n",
    "             \"g\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"going\",\"goin'\", \"gon\" ,\"gon'\" , \"gonna\" , \"gone\", \"got\", \"gotten\", \"gr\", \n",
    "             \"h\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\",  \"her\", \"here\", \"heres\", \"here's\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\",  \"ho\",  \"how\",  \"however\", \"how's\", \n",
    "             \"i\", \"ia\", \"i'd\", \"if\", \"ignored\", \"ih\", \"ii\", \"i'll\", \"im\", \"i'm\", \"in\", \"inc\", \"indeed\",  \"inner\", \"instead\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\",\n",
    "             \"j\", \"just\", \n",
    "             \"k\", \"ke\", \"keep\", \"keeps\", \"kept\",\"know\", \"known\", \"knows\", \"ko\", \n",
    "             \"l\", \"la\",\"ll\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"ll\", \"look\", \"looking\", \"looks\", \"los\", \n",
    "             \"m\", \"ma\", \"made\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\",\"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \n",
    "             \"n\", \"n't\", \"na\", \"name\", \"near\", \"nearly\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\",  \n",
    "             \"o\", \"oa\", \"obviously\", \"of\" , \"off\", \"often\", \"oh\" , \"ooh\" , \"ok\", \"okay\",\"old\",  \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \n",
    "             \"p\",\"part\", \"perhaps\", \"probably\", \n",
    "             \"q\", \"quickly\", \"quite\", \n",
    "             \"r\", \"rather\", \"readily\", \"really\", \"right\", \"run\",\n",
    "             \"s\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\",\"so\" , \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\",\"still\",  \"such\", \n",
    "             \"t\", \"take\", \"taken\", \"taking\",\"to\",\"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\",\"there's\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\",  \"truly\",  \"twelve\", \"twenty\", \"twice\", \"two\",  \n",
    "             \"u\", \"uh\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \n",
    "             \"v\", \"va\",\"very\",\n",
    "             \"w\", \"wa\", \"want\", \"wanna\" , \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \n",
    "             \"x\", \"xo\",\n",
    "             \"y\", \"yes\", \"yet\", \"yeah\" , \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \n",
    "             \"z\", \"zero\"]\n",
    "\n",
    "data['lyrics'] = data['lyrics'].str.replace(\"[-\\?.,\\/#!$%\\^&\\*;:{}=\\_~()\\`]\", ' ')\n",
    "data['lyrics'] = data['lyrics'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning : Checking Common repeated words by genre\n",
    "for the_genre in data.genre.unique():\n",
    "    print(the_genre)\n",
    "    print(data.loc[data['genre'] == the_genre].lyrics.str.split(expand=True).stack().value_counts()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize lyrics = reduce words (“stay” from “staying”)\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_lyrics(lyrics):\n",
    "    # lyrics to list of words\n",
    "    lyrics_tokens = lyrics.split()\n",
    "\n",
    "    # lemmatizing every token of every song lyrics\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    lyrics_tokens = [lemmatizer.lemmatize(token) for token in lyrics_tokens]\n",
    "        \n",
    "    # joining tokens together \n",
    "    cleaned_lyrics = \" \".join(lyrics_tokens)\n",
    "    return cleaned_lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lyrics\"]  = data[\"lyrics\"].apply(lambda x:  lemmatize_lyrics(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After lemmatizing : Checking Common repeated words by genre\n",
    "for the_genre in data.genre.unique():\n",
    "    print(the_genre)\n",
    "    print(data.loc[data['genre'] == the_genre].lyrics.str.split(expand=True).stack().value_counts()[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 CLEANED RAW DATA VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of songs grouped by genre of music:\",data.groupby('genre').count()['lyrics'])\n",
    "ax = plt.subplots()\n",
    "ax = sns.countplot(x=\"genre\", data = data, palette= \"Set1\")\n",
    "ax.set_title(\"Number of songs by genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word clouds by genre:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_genre = data.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classified = []\n",
    "for genre_lyrics in data_genre:\n",
    "    lyrics_list = []\n",
    "    for i in range(len(data.lyrics) ):\n",
    "        if (data.genre[i]==genre_lyrics):\n",
    "            lyrics_list.append(data.lyrics[i])\n",
    "    data_classified.append(lyrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "index_genre = 0 \n",
    "#iterate through every music genre\n",
    "for lyrics_of_genre in data_classified :\n",
    "\n",
    "    wordcloud = WordCloud(width = 300, height = 300,background_color ='white', min_font_size = 10).generate(\" \".join(lyrics_of_genre)+\" \")\n",
    " \n",
    "    # plot the WordCloud                       \n",
    "    plt.figure(figsize = (5, 5), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(data_genre[index_genre])\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()\n",
    "    # Save the image in the img folder:\n",
    "    #wordcloud.to_file(\"img\"+data_genre[index_genre]+\"_words.png\")\n",
    "    index_genre +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DATA MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the columns for training\n",
    "y = data.genre.values\n",
    "#x = data.lyrics.values \n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "x= cv.fit_transform(data['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB\n",
    "\n",
    "print (\"Bernoulli naive bayes: %s\"%(BernoulliNB().fit(x_train,y_train).score(x_test,y_test)))\n",
    "print (\"Multinomial naive bayes: %s\"%(MultinomialNB().fit(x_train,y_train).score(x_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics = pd.read_csv(\"https://www.kaggle.com/datasets/neisse/scrapped-lyrics-from-6-genres\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
